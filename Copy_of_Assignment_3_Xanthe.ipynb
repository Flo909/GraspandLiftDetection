{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Flo909/GraspandLiftDetection/blob/main/Copy_of_Assignment_3_Xanthe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AN4g_V-9Ty5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fbd028-a639-4e9b-9c2e-5edfe2269476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "# Training will be significantly faster if GPU is available. In Colab, go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "# set the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data:\n",
        "Download the train.zip file from https://www.kaggle.com/c/grasp-and-lift-eeg-detection/data and\n",
        "explore the dataset. This file contains the first 8 series for each subject. (We will be only using\n",
        "train.zip for the project.)\n",
        "There are two files for each subject + series combination:\n",
        "● the *_data.csv files contain the raw 32 channels EEG data (sampling rate 500Hz)\n",
        "● the *_events.csv files contains the ground truth frame-wise labels for all events\n",
        "\n",
        "NOTE: to import the data you have to log into kaggle and create an API token. From there you upload the kaggle.json file that will be downloaded when creating the API token and upload it into the connected google drive account for colab. Then give colab permission to access the files when running the below lines."
      ],
      "metadata": {
        "id": "GIE6OCntWFka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gbhk3dmXsDQ",
        "outputId": "6769fa80-3024-4294-e497-cabe6a4d0a00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYSnHGQlqkIb",
        "outputId": "31586b82-565e-484a-a44d-065c3073d30c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RaPOo89rF1O",
        "outputId": "f4c0511a-5802-49d9-e236-1a36562dbf07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "2CbmPDG_rMMU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download grasp-and-lift-eeg-detection --force"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXQveKKyrUms",
        "outputId": "7e623b01-0902-485a-fe0a-2a59a71a51e4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading grasp-and-lift-eeg-detection.zip to /content\n",
            "100% 1.02G/1.02G [00:05<00:00, 237MB/s]\n",
            "100% 1.02G/1.02G [00:05<00:00, 214MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile('grasp-and-lift-eeg-detection.zip', 'r') as zip_ref:\n",
        "    # Extract all files\n",
        "    zip_ref.extractall('grasp-and-lift-eeg-detection')\n",
        "\n",
        "# List the extracted files\n",
        "extracted_files = zip_ref.namelist()\n",
        "\n",
        "display(extracted_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "SFO8oHjqtK1p",
        "outputId": "89d2cf44-fbe1-4018-8c49-08c10c4a6aac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['sample_submission.csv.zip', 'test.zip', 'train.zip']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracted folder name will be the same as the zip file name without the extension\n",
        "extracted_folder = os.path.splitext('grasp-and-lift-eeg-detection/train.zip')[0]\n",
        "\n",
        "with zipfile.ZipFile('grasp-and-lift-eeg-detection/train.zip', 'r') as zip_ref:\n",
        "    # Extract all files\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "extracted_files = zip_ref.namelist()"
      ],
      "metadata": {
        "id": "Gx4-o-yw7Nqo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read data and labels\n",
        "def read_data_and_labels(extracted_files, extracted_folder):\n",
        "    train_data = []\n",
        "    test_data = []\n",
        "    test_labels = []\n",
        "    train_labels = []\n",
        "\n",
        "    for file_name in extracted_files:\n",
        "        if file_name.endswith('_data.csv'):\n",
        "            subject_id, series = file_name.split('_')[:2]\n",
        "            df = pd.read_csv(os.path.join(extracted_folder, file_name))\n",
        "            df.drop(columns = ['id'], inplace=True)\n",
        "            if series == 'series7' or series == 'series8':\n",
        "                test_data.append(df.T.astype(np.float32))\n",
        "            else:\n",
        "                train_data.append(df.T.astype(np.float32))\n",
        "\n",
        "        elif file_name.endswith('_events.csv'):\n",
        "            subject_id, series = file_name.split('_')[:2]\n",
        "            labels_df = pd.read_csv(os.path.join(extracted_folder, file_name))\n",
        "            labels_df.drop(columns =['id'], inplace=True)\n",
        "            if series == 'series7' or series == 'series8':\n",
        "              test_labels.append(labels_df.T.astype(np.float32))\n",
        "            else:\n",
        "              train_labels.append(labels_df.T.astype(np.float32))\n",
        "\n",
        "\n",
        "    return train_data, test_data, test_labels, train_labels\n",
        "\n",
        "# Read data and labels\n",
        "train_data, test_data, test_labels, train_labels = read_data_and_labels(extracted_files, extracted_folder)\n",
        "\n"
      ],
      "metadata": {
        "id": "F1q3GFzEvpvS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset class\n",
        "Note: example gets rid of long chunks of data with no events-> could be something to look into"
      ],
      "metadata": {
        "id": "8frS5iWDIp8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom Dataset Class\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data, labels, in_len, train=True):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.in_len = in_len\n",
        "\n",
        "        self.index = [(i, j) for i in range(len(data)) for j in range(data[i].shape[1])]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i, j = self.index[idx]\n",
        "\n",
        "        data, labels = self.data[i][:, max(0, j - self.in_len + 1):j + 1], self.labels[i][:, j]\n",
        "\n",
        "        data, label = torch.from_numpy(data.astype(np.float32)), torch.from_numpy(labels.astype(np.float32))\n",
        "        return data, label\n",
        "\n",
        "# Create Dataset instances\n",
        "# final argument must be an ingeger between 1 and 33 -> the larger the number the more computationally complex\n",
        "in_len = 15\n",
        "train_dataset = EEGDataset(train_data, train_labels, in_len)\n",
        "test_dataset = EEGDataset(test_data, test_labels, in_len)\n",
        "\n",
        "dataLoader = DataLoader(train_dataset, batch_size=32)\n",
        "\n"
      ],
      "metadata": {
        "id": "6qHzDGkGiEQf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.shape)"
      ],
      "metadata": {
        "id": "g1CWDR2CUAlv",
        "outputId": "ffea135f-317b-475e-8bc5-c33841e8d4ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'EEGDataset' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-115632d0b38a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'EEGDataset' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Neural Network\n"
      ],
      "metadata": {
        "id": "ElqXn3ENI4-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN architecture\n",
        "class EEGCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EEGCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(32 * 125, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 125)  # Flattening\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "m8A3ZayRI74o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define model\n",
        "model = EEGCNN().to(device)\n",
        "\n",
        "# Define your loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define your optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Set the number of epochs\n",
        "num_epochs = 20\n",
        "\n",
        "# Lists to store the training and validation losses and accuracies\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize the training loss and accuracy for this epoch\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for inputs, labels in dataLoader:\n",
        "\n",
        "        # Move the inputs and labels to the GPU if available\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, labels_reshape = torch.max(labels, 1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_total += labels_reshape.size(0)\n",
        "        train_correct += (predicted == labels_reshape).sum().item()\n",
        "\n",
        "\n",
        "    # Compute the average training loss and accuracy for this epoch\n",
        "    train_loss /= len(dataLoader.dataset)\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "    # Append the training loss and accuracy to the lists\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize the validation loss and accuracy for this epoch\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    # EXERCISE - Calculate the validation loss and accuracy and append to the lists\n",
        "\n",
        "    # Compute the average validation loss and accuracy for this epoch\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "    # Append the validation loss and accuracy to the lists\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print the training and validation metrics for this epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "lymxtOwfKvUS",
        "outputId": "8ee76066-fed1-40ee-a6be-5ce674b9485e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "(slice(None, None, None), 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4b37cd22a4db>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Iterate over the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataLoader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Move the inputs and labels to the GPU if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-979f4e811f27>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHashable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (slice(None, None, None), 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Filtering\n",
        "Filtering the Data\n",
        "(maybe do this after we actually have a nn)\n"
      ],
      "metadata": {
        "id": "ZXR0aQrGfd9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "\n",
        "# Making a copy of the dataset so as the maintain the original\n",
        "filtered_train = train_dataset\n",
        "filtered_test = test_dataset\n",
        "\n",
        "def filter_eeg_dataset(EEGDataset, f_low, f_high, order, type, fs):\n",
        "  b, a = signal.butter(order, [f_low, f_high], btype=type, fs=fs)\n",
        "\n",
        "  subject_names = EEGDataset.subject_names\n",
        "\n",
        "  # For each Subject in EEG Dataset\n",
        "  for subject in subject_names:\n",
        "    # For each Series in Subject\n",
        "    for series in range(6):\n",
        "      try:\n",
        "        data_trial = EEGDataset.subjects[subject][series]\n",
        "        # For each Column in series and subject\n",
        "        # Not the first non-numerical columns\n",
        "        for col_name in data_trial.columns:\n",
        "\n",
        "          if col_name != \"id\":\n",
        "            filtered_data = signal.filtfilt(b, a, data_trial[col_name])\n",
        "\n",
        "            # Replace data\n",
        "            EEGDataset.subjects[subject][series][col_name] = filtered_data\n",
        "\n",
        "\n",
        "      # Breaking when we run out of series\n",
        "      except IndexError as e:\n",
        "        print(str(subject) + \" caps at \"+ str(series))\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# System Params\n",
        "order = 4\n",
        "sampling_frequency = 500\n",
        "\n",
        "# High and Low Pass Filter\n",
        "f_low = 1\n",
        "f_high = 100\n",
        "f_type = 'bandpass'\n",
        "\n",
        "filter_eeg_dataset(filtered_train, f_low, f_high, order, f_type, sampling_frequency)\n",
        "filter_eeg_dataset(filtered_test, f_low, f_high, order, f_type, sampling_frequency)\n",
        "\n",
        "# Bandreject\n",
        "f_low = 49\n",
        "f_high = 51\n",
        "f_type = 'bandstop'\n",
        "\n",
        "filter_eeg_dataset(filtered_train, f_low, f_high, order, f_type, sampling_frequency)\n",
        "filter_eeg_dataset(filtered_test, f_low, f_high, order, f_type, sampling_frequency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2k5pnDTffke",
        "outputId": "434aba82-e42a-4e32-f2c4-97f9539db099"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train/subj10 caps at 2\n",
            "train/subj11 caps at 2\n",
            "train/subj12 caps at 2\n",
            "train/subj1 caps at 2\n",
            "train/subj2 caps at 2\n",
            "train/subj3 caps at 2\n",
            "train/subj4 caps at 2\n",
            "train/subj5 caps at 2\n",
            "train/subj6 caps at 2\n",
            "train/subj7 caps at 2\n",
            "train/subj8 caps at 2\n",
            "train/subj9 caps at 2\n",
            "train/subj10 caps at 2\n",
            "train/subj11 caps at 2\n",
            "train/subj12 caps at 2\n",
            "train/subj1 caps at 2\n",
            "train/subj2 caps at 2\n",
            "train/subj3 caps at 2\n",
            "train/subj4 caps at 2\n",
            "train/subj5 caps at 2\n",
            "train/subj6 caps at 2\n",
            "train/subj7 caps at 2\n",
            "train/subj8 caps at 2\n",
            "train/subj9 caps at 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Extraction\n"
      ],
      "metadata": {
        "id": "lKDQGVI6fgLE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiVSDcvjfiWT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}