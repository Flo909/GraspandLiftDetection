{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Flo909/GraspandLiftDetection/blob/main/Copy_of_Assignment_3_Florian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN4g_V-9Ty5f",
        "outputId": "a8fdf95d-fece-4be4-a1b6-14f2ca5a065f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is not available\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "run_on_colab = True\n",
        "\n",
        "# Training will be significantly faster if GPU is available. In Colab, go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "# set the device to GPU if available, else CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIE6OCntWFka"
      },
      "source": [
        "Import data:\n",
        "Download the train.zip file from https://www.kaggle.com/c/grasp-and-lift-eeg-detection/data and\n",
        "explore the dataset. This file contains the first 8 series for each subject. (We will be only using\n",
        "train.zip for the project.)\n",
        "There are two files for each subject + series combination:\n",
        "● the *_data.csv files contain the raw 32 channels EEG data (sampling rate 500Hz)\n",
        "● the *_events.csv files contains the ground truth frame-wise labels for all events\n",
        "\n",
        "NOTE: to import the data you have to log into kaggle and create an API token. From there you upload the kaggle.json file that will be downloaded when creating the API token and upload it into the connected google drive account for colab. Then give colab permission to access the files when running the below lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gbhk3dmXsDQ",
        "outputId": "9d8525b5-17e2-4415-fb87-e047ac2a63dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.4)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ],
      "source": [
        "if (run_on_colab):\n",
        "  ! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYSnHGQlqkIb",
        "outputId": "0e8338d5-a455-428a-da68-35e40e0bd9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if (run_on_colab):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  ! mkdir ~/.kaggle\n",
        "  ! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXQveKKyrUms",
        "outputId": "a25bb80f-4a33-40cd-fb00-81590af6bd97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading grasp-and-lift-eeg-detection.zip to /content\n",
            " 99% 1.01G/1.02G [00:13<00:00, 57.1MB/s]\n",
            "100% 1.02G/1.02G [00:13<00:00, 82.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "! kaggle competitions download grasp-and-lift-eeg-detection --force"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "SFO8oHjqtK1p",
        "outputId": "e15e381d-85f7-40ff-b525-de34c885964a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['sample_submission.csv.zip', 'test.zip', 'train.zip']"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile('grasp-and-lift-eeg-detection.zip', 'r') as zip_ref:\n",
        "    # Extract all files\n",
        "    zip_ref.extractall('grasp-and-lift-eeg-detection')\n",
        "\n",
        "# List the extracted files\n",
        "extracted_files = zip_ref.namelist()\n",
        "\n",
        "display(extracted_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gx4-o-yw7Nqo"
      },
      "outputs": [],
      "source": [
        "# Extracted folder name will be the same as the zip file name without the extension\n",
        "extracted_folder = os.path.splitext('grasp-and-lift-eeg-detection/train.zip')[0]\n",
        "\n",
        "with zipfile.ZipFile('grasp-and-lift-eeg-detection/train.zip', 'r') as zip_ref:\n",
        "    # Extract all files\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "extracted_files = zip_ref.namelist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F1q3GFzEvpvS"
      },
      "outputs": [],
      "source": [
        "# Function to read data and labels\n",
        "def read_data_and_labels(extracted_files, extracted_folder):\n",
        "    train_data = []\n",
        "    val_data = []\n",
        "    test_labels = []\n",
        "    train_labels = []\n",
        "\n",
        "    for file_name in extracted_files:\n",
        "        if file_name.endswith('_data.csv'):\n",
        "            subject_id, series = file_name.split('_')[:2]\n",
        "            df = pd.read_csv(os.path.join(extracted_folder, file_name))\n",
        "            df.drop(columns = ['id'], inplace=True)\n",
        "            if series == 'series7' or series == 'series8':\n",
        "                val_data.append(df.T.astype(np.float32))\n",
        "            else:\n",
        "                train_data.append(df.T.astype(np.float32))\n",
        "\n",
        "        elif file_name.endswith('_events.csv'):\n",
        "            subject_id, series = file_name.split('_')[:2]\n",
        "            labels_df = pd.read_csv(os.path.join(extracted_folder, file_name))\n",
        "            labels_df.drop(columns =['id'], inplace=True)\n",
        "            if series == 'series7' or series == 'series8':\n",
        "              test_labels.append(labels_df.T.astype(np.float32))\n",
        "            else:\n",
        "              train_labels.append(labels_df.T.astype(np.float32))\n",
        "\n",
        "\n",
        "    return train_data, val_data, test_labels, train_labels\n",
        "\n",
        "# Read data and labels\n",
        "train_data, val_data, test_labels, train_labels = read_data_and_labels(extracted_files, extracted_folder)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8frS5iWDIp8E"
      },
      "source": [
        "# Create dataset class\n",
        "Note: example gets rid of long chunks of data with no events-> could be something to look into"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6qHzDGkGiEQf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom Dataset Class\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data, labels, in_len, train=True):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.in_len = in_len\n",
        "\n",
        "        self.index = [(i, j) for i in range(len(data)) for j in range(data[i].shape[1])]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index)\n",
        "        #return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        i, j = self.index[idx]\n",
        "\n",
        "        data, labels = self.data[i].iloc[:,max(0, j - self.in_len + 1):j + 1], self.labels[i].iloc[:,j]\n",
        "        #print('checkpoint')\n",
        "        data_array = np.array(data, dtype=np.float32)\n",
        "        labels_array = np.array(labels, dtype=np.float32)\n",
        "\n",
        "        if (data_array.shape[1] < self.in_len):\n",
        "          data_array = np.pad(data_array, ((0,0),(self.in_len-data.shape[1],0)),mode='edge')#, constant_values=np.float32(0.0))\n",
        "\n",
        "        data, label = torch.from_numpy(data_array.astype(np.float32)), torch.from_numpy(labels_array.astype(np.float32))\n",
        "        #data, label = torch.from_numpy(np.array(data, dtype=np.float32)), torch.from_numpy(np.array(labels, dtype=np.float32))\n",
        "        return data, label\n",
        "\n",
        "# Create Dataset instances\n",
        "# final argument must be an ingeger between 1 and 33 -> the larger the number the more computationally complex\n",
        "in_len = 64\n",
        "train_dataset = EEGDataset(train_data, train_labels, in_len)\n",
        "val_dataset = EEGDataset(val_data, test_labels, in_len)\n",
        "\n",
        "data_loader = DataLoader(train_dataset, batch_size=1024)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1024)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g1CWDR2CUAlv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "79939de8-bfc9-4f1a-b440-0f7b734b51c8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'module' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8e19e9f0d3f5>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#print(train_dataset[500][0].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mmax_pool1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_pool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ],
      "source": [
        "#print(train_dataset.shape)\n",
        "\"\"\"\n",
        "for i in range(len(train_data)):\n",
        "  #print (train_data[i].shape[1])\n",
        "  if (train_data[i].shape[1] != train_labels[i].shape[1]):\n",
        "    print(\"error match\")\n",
        "  if (train_data[i].shape[0] != 32):\n",
        "    print(\"error dim\")\n",
        "\n",
        "for i in range(len(train_labels)):\n",
        "  print (train_labels[i].shape[1])\n",
        "  if (train_labels[i].shape[0] != 6):\n",
        "    print(\"error dim\")\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "nested_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
        "nested_array = np.array(nested_list)\n",
        "print(nested_array)\n",
        "\n",
        "\n",
        "#print(type(train_data[0]))\n",
        "#print(train_data[0].iloc[:,0:2])\n",
        "\n",
        "print(train_dataset[100][0].shape)\n",
        "print(len(train_dataset))\n",
        "for inputs, labels in dataLoader:\n",
        "    print(\"Input shape:\", inputs.shape)\n",
        "    print(\"Labels shape:\", labels.shape)\n",
        "    break\n",
        "#print(train_dataset[]\n",
        "#print([(i, j) for i in range(len(train_data)) for j in range(train_data[i].shape[1])])\n",
        "#print(train_data[0][1][0])\n",
        "#print([len(train_data[0][i]) for i in range(len(train_data[0]))])\n",
        "\n",
        "ARchitectures to look at\n",
        "1)Inception\n",
        "2)Densenets\n",
        "3)Resnets\n",
        "\"\"\"\n",
        "#print(train_dataset[500][0].shape)\n",
        "max_pool1 = nn.MaxPool1d(8,1)\n",
        "print(max_pool1(torch.from_numpy(np.random(16,1))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElqXn3ENI4-B"
      },
      "source": [
        "# Create a Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "m8A3ZayRI74o"
      },
      "outputs": [],
      "source": [
        "# Define the CNN architecture\n",
        "class EEGCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        self.channels = 32\n",
        "        self.outchannels = 32\n",
        "        super(EEGCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels= self.channels, out_channels= self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels)\n",
        "        self.conv2 = nn.Conv1d(in_channels= self.channels, out_channels= self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels)\n",
        "        self.conv3 = nn.Conv1d(in_channels= self.channels, out_channels= self.channels, kernel_size=3, stride=1, padding=1, groups=self.channels)\n",
        "        #self.conv2 = nn.Conv1d(in_channels= self.inchannels, out_channels= self.outchannels, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.pool_large = nn.MaxPool1d(kernel_size=8, stride=8)\n",
        "        self.fc1 = nn.Linear(self.channels*2, 6)\n",
        "        #self.fc1 = nn.Linear(32 * 125, 128)\n",
        "        self.fc2 = nn.Linear(128, 6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #64 -> 32\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        #32 -> 16\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        #16 -> 2\n",
        "        #print('check1')\n",
        "        x = self.pool_large(torch.relu(self.conv3(x)))\n",
        "        #x = self.pool(torch.relu(self.conv1(x)))\n",
        "        #print(x.shape)\n",
        "        #x = self.pool(x)\n",
        "        x = x.view(-1, self.channels * 2)  # Flattening\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        #x = self.fc2(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqbSCF8FTok1"
      },
      "outputs": [],
      "source": [
        "def dataloader_validation(loader):\n",
        "  input, label = loader.dataset[0]\n",
        "  print(label)\n",
        "  print(input)\n",
        "  return\n",
        "\n",
        "#dataloader_validation(dataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lymxtOwfKvUS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4abc9d0-5147-4c9a-e6dd-a8f15679887e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 225/14473 [01:11<1:36:12,  2.47it/s]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define model\n",
        "model = EEGCNN().to(device)\n",
        "\n",
        "# Define your loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define your optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
        "\n",
        "# Set the number of epochs\n",
        "num_epochs = 20\n",
        "\n",
        "# Lists to store the training and validation losses and accuracies\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize the training loss and accuracy for this epoch\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for inputs, labels in tqdm(data_loader):\n",
        "\n",
        "        # Move the inputs and labels to the GPU if available\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        #print(inputs.shape)\n",
        "        #print(outputs.shape)\n",
        "        #print(labels.shape)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "        # Update the training loss and accuracy\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "        _, labels_reshape = torch.max(labels, 1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_total += labels_reshape.size(0)\n",
        "        train_correct += (predicted == labels_reshape).sum().item()\n",
        "\n",
        "\n",
        "    # Compute the average training loss and accuracy for this epoch\n",
        "    train_loss /= len(data_loader.dataset)\n",
        "    train_accuracy = 100 * train_correct / train_total\n",
        "\n",
        "    # Append the training loss and accuracy to the lists\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize the validation loss and accuracy for this epoch\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    # Calculate the validation loss and accuracy and append to the lists\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in tqdm(val_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        val_loss += loss.item() * inputs.size(0)\n",
        "        _, labels_reshape = torch.max(labels, 1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        val_total += labels_reshape.size(0)\n",
        "        val_correct += (predicted == labels_reshape).sum().item()\n",
        "\n",
        "    # Compute the average validation loss and accuracy for this epoch\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "\n",
        "    # Append the validation loss and accuracy to the lists\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print the training and validation metrics for this epoch\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.2f}%\")\n",
        "    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.2f}%\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXR0aQrGfd9l"
      },
      "source": [
        "#Filtering\n",
        "Filtering the Data\n",
        "(maybe do this after we actually have a nn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2k5pnDTffke"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "\n",
        "# Making a copy of the dataset so as the maintain the original\n",
        "filtered_train = train_dataset\n",
        "filtered_test = val_dataset\n",
        "\n",
        "def filter_eeg_dataset(EEGDataset, f_low, f_high, order, type, fs):\n",
        "  b, a = signal.butter(order, [f_low, f_high], btype=type, fs=fs)\n",
        "\n",
        "  subject_names = EEGDataset.subject_names\n",
        "\n",
        "  # For each Subject in EEG Dataset\n",
        "  for subject in subject_names:\n",
        "    # For each Series in Subject\n",
        "    for series in range(6):\n",
        "      try:\n",
        "        data_trial = EEGDataset.subjects[subject][series]\n",
        "        # For each Column in series and subject\n",
        "        # Not the first non-numerical columns\n",
        "        for col_name in data_trial.columns:\n",
        "\n",
        "          if col_name != \"id\":\n",
        "            filtered_data = signal.filtfilt(b, a, data_trial[col_name])\n",
        "\n",
        "            # Replace data\n",
        "            EEGDataset.subjects[subject][series][col_name] = filtered_data\n",
        "\n",
        "\n",
        "      # Breaking when we run out of series\n",
        "      except IndexError as e:\n",
        "        print(str(subject) + \" caps at \"+ str(series))\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# System Params\n",
        "order = 4\n",
        "sampling_frequency = 500\n",
        "\n",
        "# High and Low Pass Filter\n",
        "f_low = 1\n",
        "f_high = 100\n",
        "f_type = 'bandpass'\n",
        "\n",
        "filter_eeg_dataset(filtered_train, f_low, f_high, order, f_type, sampling_frequency)\n",
        "filter_eeg_dataset(filtered_test, f_low, f_high, order, f_type, sampling_frequency)\n",
        "\n",
        "# Bandreject\n",
        "f_low = 49\n",
        "f_high = 51\n",
        "f_type = 'bandstop'\n",
        "\n",
        "filter_eeg_dataset(filtered_train, f_low, f_high, order, f_type, sampling_frequency)\n",
        "filter_eeg_dataset(filtered_test, f_low, f_high, order, f_type, sampling_frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKDQGVI6fgLE"
      },
      "source": [
        "Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiVSDcvjfiWT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}